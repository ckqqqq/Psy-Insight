{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os\n",
                "# import wandb\n",
                "# os.environ['WANDB_USERNAME']=\"qiker-chen\"\n",
                "# wandb.login(key=\"69fdb45d404436728fa8b67771c5e3b4b94e6002\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path=\"/home/ckqsudo/code2024/0model/Mistral-7B-Instruct-v0.2\"\n",
                "import os\n",
                "os.environ[\"WANDB_API_KEY\"]=\"284e19ed3e365d69a51b9c3b4d61aa9f72dfa2ed\"\n",
                "os.environ[\"ALL_PROXY\"]=\"127.0.0.1:11137\"\n",
                "import wandb\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "# http://localhost:8888/tree?token=6ef876332060f7c3d066571cd2918641b875b9cd557cac0e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
                "# http://localhost:8888/tree?token=6ef876332060f7c3d066571cd2918641b875b9cd557cac0e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "dataset_arg=\"_continue_training\"\n",
                "# /dataset_arg=\"+reasoning\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "dc12157d50b04175b13475d9c962800e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1af2aaa5910e46edb232c2f3261d1518",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "658e41f1c5d94a16be418e14d600f7e5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "88e5ed74ff6b44f593e73deb640a1864",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "_continue_training_continue_training_continue_training_continue_training\n"
                    ]
                }
            ],
            "source": [
                "from random import shuffle\n",
                "from datasets import load_from_disk\n",
                "from datasets import load_dataset\n",
                "# Load jsonl data from disk\n",
                "from datasets import DatasetDict\n",
                "from datasets import Dataset\n",
                "from datasets import concatenate_datasets\n",
                "dataset_values=[\"_observation_reasoning\",\"_reasoning\",\"_pure_dialog\",\"_pure_reasoning_observation\",\"_continue_training\"]\n",
                "\n",
                "if dataset_arg not in dataset_values:\n",
                "    raise ValueError()\n",
                "if dataset_arg==\"_observation_reasoning\":\n",
                "    en_dataset=load_from_disk(\"/home/ckqsudo/code2024/Mistral-7b-finetune/hf_en_observation_reasoning_dataset_3\")\n",
                "    dataset=en_dataset.train_test_split(0.15,shuffle=True,seed=66)\n",
                "elif dataset_arg==\"_reasoning\":\n",
                "    en_dataset=load_from_disk(\"/home/ckqsudo/code2024/Mistral-7b-finetune/hf_en_reasoning_dataset_3\")\n",
                "    dataset=en_dataset.train_test_split(0.15,shuffle=True,seed=66)\n",
                "elif dataset_arg in [\"_pure_dialog\",\"_continue_training\"]:\n",
                "    en_dataset=load_from_disk(\"/home/ckqsudo/code2024/Mistral-7b-finetune/hf_en_pure_dialog_dataset_3\")\n",
                "    dataset=en_dataset.train_test_split(0.15,shuffle=True,seed=66)\n",
                "elif dataset_arg==\"_pure_reasoning_observation\":\n",
                "    en_obs=load_from_disk(\"/home/ckqsudo/code2024/Mistral-7b-finetune/hf_en_pure_observation_dataset_3\")\n",
                "    en_rea=load_from_disk(\"/home/ckqsudo/code2024/Mistral-7b-finetune/hf_en_pure_reasoning_dataset_3\")\n",
                "    obs_dataset=en_obs.train_test_split(0.15,shuffle=True,seed=66)\n",
                "    rea_dataset=en_rea.train_test_split(0.15,shuffle=True,seed=66)\n",
                "    en_dataset = concatenate_datasets(dsets=[obs_dataset['train'],rea_dataset['train']]).shuffle(66)\n",
                "    print(en_dataset)\n",
                "    dataset=DatasetDict()\n",
                "    dataset[\"train\"],dataset[\"test\"]=en_dataset,concatenate_datasets(dsets=[obs_dataset['test'],rea_dataset['test']])\n",
                "else:\n",
                "    raise ValueError()\n",
                "dataset[\"train\"].to_json(f\"train_and_test_data/train_dataset{dataset_arg}.jsonl\",orient=\"records\")\n",
                "dataset[\"test\"].to_json(f\"train_and_test_data/test_dataset{dataset_arg}.jsonl\", orient=\"records\")\n",
                "train_dataset = load_dataset(\"json\",data_files=f\"train_and_test_data/train_dataset{dataset_arg}.jsonl\")\n",
                "test_dataset= load_dataset(\"json\",data_files=f\"train_and_test_data/test_dataset{dataset_arg}.jsonl\")\n",
                "print(dataset_arg+dataset_arg+dataset_arg+dataset_arg)\n",
                "\n",
                "# test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'_continue_training'"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset_arg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[DatasetDict({\n",
                            "     train: Dataset({\n",
                            "         features: ['messages'],\n",
                            "         num_rows: 456\n",
                            "     })\n",
                            " }),\n",
                            " DatasetDict({\n",
                            "     train: Dataset({\n",
                            "         features: ['messages'],\n",
                            "         num_rows: 81\n",
                            "     })\n",
                            " })]"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "[train_dataset,test_dataset]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "# def valid_messages_format(tokenizer,train_dataset,test_dataset):\n",
                "#     check_list=[]\n",
                "#     dataset_dict_list=[train_dataset,test_dataset]\n",
                "#     for ds_dict in dataset_dict_list:\n",
                "#         for item in ds_dict['train']['messages']:\n",
                "#             check_str=\"\"\n",
                "#             for idx,dialog_item in enumerate(item):\n",
                "#                 check_str+=dialog_item[\"role\"]\n",
                "#             tokenizer.apply_chat_template(item, tokenize=False)\n",
                "#             if check_str.startswith('user')==False:\n",
                "#                 print(check_str)\n",
                "#             if 'useruser' in check_str:\n",
                "#                 print(check_str)\n",
                "# valid_messages_format(tokenizer,train_dataset,test_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# token_ids = tokenizer(\"[INST]\", add_special_tokens=False)\n",
                "# print(token_ids['input_ids'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path=\"/home/ckqsudo/code2024/0model/Mistral-7B-Instruct-v0.2\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "merge_model\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b1326ce90896451e91afb993b2977f49",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4dc8fc4903b1487b9f376a6a525ddebb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from trl import setup_chat_format\n",
                "from peft import LoraConfig, PeftModel\n",
                "# tokenizer = AutoTokenizer.from_pretrained(model_path,padding=\"longest\")\n",
                "# Hugging Face model id\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "# Load model and tokenizer\n",
                "\n",
                "# Load model and tokenizer\n",
                "\n",
                "\n",
                "if dataset_arg=='_continue_training':\n",
                "    print(\"merge_model\")\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_path,\n",
                "        low_cpu_mem_usage=True,\n",
                "        return_dict=True,\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        device_map=\"auto\")   \n",
                "    merged_model= PeftModel.from_pretrained(base_model,model_id=\"/home/ckqsudo/code2024/Mistral-7b-finetune/trl_standard_output/fuck8_6_8_unpacking__pure_reasoning_observation/checkpoint-210\")\n",
                "    merged_model = merged_model.merge_and_unload()\n",
                "    merged_model.save_pretrained(\"/home/ckqsudo/code2024/Mistral-7b-finetune/trl_standard_output/merge_model_6_11/merge_model\")\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        \"/home/ckqsudo/code2024/Mistral-7b-finetune/trl_standard_output/merge_model_6_8/merge_model\",\n",
                "        device_map=\"auto\",\n",
                "        attn_implementation=\"flash_attention_2\",\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        quantization_config=bnb_config\n",
                "    )\n",
                "else:\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_path,\n",
                "        device_map=\"auto\",\n",
                "        attn_implementation=\"flash_attention_2\",\n",
                "        torch_dtype=torch.bfloat16,\n",
                "        quantization_config=bnb_config\n",
                "    )\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_path,padding=\"longest\")\n",
                "# tokenizer = AutoTokenizer.from_pretrained(model_path,truncation=True,padding_side = 'right',padding=\"longest\",padding_side='right')\n",
                "\n",
                "# tokenizer.padding_side = 'right' # to prevent warnings\n",
                "# tokenizer.pad_token = tokenizer.unk_token # 消融实验"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('/home/ckqsudo/code2024/0model/Mistral-7B-Instruct-v0.2',\n",
                            " LlamaTokenizerFast(name_or_path='/home/ckqsudo/code2024/0model/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
                            " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
                            " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
                            " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
                            " })"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_path,tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('left', None, None, 0)"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "tokenizer.padding_side,tokenizer.pad_token_id,tokenizer.pad_token,tokenizer.unk_token_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "456"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(train_dataset['train'][\"messages\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "# tokenizer.pad_token_id=-1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "from peft import LoraConfig\n",
                "\n",
                "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
                "\n",
                "peft_config = LoraConfig(\n",
                "    lora_alpha=128,\n",
                "    lora_dropout=0.05,\n",
                "    r=256,\n",
                "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\",\"lm_head\"],\n",
                "    # target_modules=\"all-linear\",有些情况下可以这样设置参数，但是一般有些问题\n",
                "    bias=\"none\",\n",
                "    task_type=\"causal_lm\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(<PaddingStrategy.DO_NOT_PAD: 'do_not_pad'>,\n",
                            " <TruncationStrategy.DO_NOT_TRUNCATE: 'do_not_truncate'>,\n",
                            " None,\n",
                            " {})"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# dataset[\"train\"][0]\n",
                "tokenizer._get_padding_truncation_strategies()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(456,\n",
                            " 81,\n",
                            " DatasetDict({\n",
                            "     train: Dataset({\n",
                            "         features: ['messages'],\n",
                            "         num_rows: 456\n",
                            "     })\n",
                            "     test: Dataset({\n",
                            "         features: ['messages'],\n",
                            "         num_rows: 81\n",
                            "     })\n",
                            " }),\n",
                            " str)"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "a=tokenizer.apply_chat_template(dataset['train'][0]['messages'], tokenize=False)\n",
                "len(dataset['train']),len(dataset['test']),dataset,type(a)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"超棒的trl预处理函数\"\"\"\n",
                "\"\"\"好用的代码来自 https://github.com/sids07/LLM_finetuning/blob/dafacc3ed04016eb54ad846793c405e97f7e1d5f/trainer-api/utils.py#L54 好用的代码\"\"\"\n",
                "from datasets import DatasetDict, load_dataset\n",
                "from transformers import DataCollatorForLanguageModeling\n",
                "from trl import DataCollatorForCompletionOnlyLM\n",
                "import numpy as np\n",
                "from functools import partial\n",
                "\n",
                "def preprocess_batch(batch, tokenizer, max_length):\n",
                "    \"\"\"Preprocess a batch of inputs for the language model.\"\"\"\n",
                "    batch[\"input_ids\"] = tokenizer(batch[\"text\"], max_length=max_length, truncation=True).input_ids\n",
                "    return batch\n",
                "\n",
                "def preprocess_dataset(dataset, tokenizer, max_length= 4096, seed=66):\n",
                "    # partial 是 Python 中的一个内置函数，它用于部分应用一个函数，即固定函数的某些参数，返回一个新的函数。\n",
                "    _preprocessing_function = partial(\n",
                "        preprocess_batch, max_length=max_length, tokenizer=tokenizer\n",
                "    )\n",
                "    # 对数据集的每列做这种映射\n",
                "    ckq_dataset = dataset.map(\n",
                "    lambda examples: {\n",
                "        'text': tokenizer.apply_chat_template(conversation=examples['messages'], tokenize=False, add_generation_prompt=False)\n",
                "    }\n",
                "    )\n",
                "    dataset = ckq_dataset.map(\n",
                "        _preprocessing_function,\n",
                "        batched=True,\n",
                "        remove_columns=['messages', \"text\"],\n",
                "    )\n",
                "    # trl 最坑的地方必须要丢弃所有列\n",
                "\n",
                "    dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
                "    dataset = dataset.shuffle(seed=seed)\n",
                "# https://github.com/sids07/LLM_finetuning/blob/dafacc3ed04016eb54ad846793c405e97f7e1d5f/trainer-api/utils.py#L54 好用的代码\n",
                "    return dataset\n",
                "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
                "trl_train_dataset=preprocess_dataset(dataset['train'],tokenizer)\n",
                "trl_test_dataset=preprocess_dataset(dataset['test'],tokenizer)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'input_ids': tensor([[32000, 32000, 32000,  ..., 28748, 16289, 28793],\n",
                        "        [32000, 32000, 32000,  ..., 28748, 16289, 28793],\n",
                        "        [32000, 32000, 32000,  ..., 14120, 28804,     2],\n",
                        "        ...,\n",
                        "        [32000, 32000, 32000,  ...,  3519, 28804,     2],\n",
                        "        [32000, 32000, 32000,  ..., 28748, 16289, 28793],\n",
                        "        [32000, 32000, 32000,  ...,  1753, 28804,     2]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
                        "        [0, 0, 0,  ..., 1, 1, 1],\n",
                        "        [0, 0, 0,  ..., 1, 1, 1],\n",
                        "        ...,\n",
                        "        [0, 0, 0,  ..., 1, 1, 1],\n",
                        "        [0, 0, 0,  ..., 1, 1, 1],\n",
                        "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
                        "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
                        "        [ -100,  -100,  -100,  ..., 14120, 28804,     2],\n",
                        "        ...,\n",
                        "        [ -100,  -100,  -100,  ...,  3519, 28804,     2],\n",
                        "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
                        "        [ -100,  -100,  -100,  ...,  1753, 28804,     2]])}\n",
                        "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "        28738,   663,   377,   392, 28747, 19393, 28804,  1824,   622,   369,\n",
                        "          913,   737, 28804,  1047,   315,   654,   298,  1032,   368,  1404,\n",
                        "          778,   574,  9585,  3667, 28725,   910,   682,   315,   873,   368,\n",
                        "          460,   438, 28705, 28770, 28804,     2,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100, 28738,   663,   377,   392, 28747,\n",
                        "        24527, 28723,  9377,   369,  4804,  4662,  1055, 28804,     2,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "        28738,   663,   377,   392, 28747,   351, 28809, 14112, 28725,  2997,\n",
                        "          473, 28747,   863,   368,   873,   369,  3198, 28708, 12477,   369,\n",
                        "         4662, 28804,     2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100, 28738,   663,   377,   392, 28747,   382,  3221, 28719,\n",
                        "        28723,   842,   842,  6926,   511,   368,  1073,   368,   989,   622,\n",
                        "          347,   739,  3198, 28708,   349,  7887,   298, 28705, 28770, 28804,\n",
                        "        28705, 18862, 28723, 15573,   622,   347,  1873, 28723,   315, 28809,\n",
                        "          584,  1601,  1873, 28723,   315,  1601,  1873,  2141, 28723,     2,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
                        "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]) tensor([32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
                        "        32000, 32000,     1,     1,   733, 16289, 28793,   995,   460,   264,\n",
                        "        10131,   304,  5024, 16107,  7140,  1039,   377,   392,   693,  1743,\n",
                        "         5312,  5857, 10526,  1760, 28723,  1387,   349,   264,  1353, 18835,\n",
                        "        14625,   288,  1222, 28723,  1387,   349,   264,  5414, 28747,  2997,\n",
                        "          473,  4072,   274, 10864,   684,   559,  5690,  3198, 28708,   304,\n",
                        "          272,  4870,  6727,   302, 27453,   684,   351, 28742, 14112, 28723,\n",
                        "         5919,  8270,   272,  2899,   302,   272, 10436, 28723,  1387,   349,\n",
                        "          272, 10436,  3340, 28747,  2188, 28747, 28743,  3428, 28747,  8352,\n",
                        "        28725,  4662, 28723, 13892, 28747, 28738,   663,   377,   392, 28747,\n",
                        "         1537, 28725,   368, 28809,   267,   438, 28705, 28750,   304,   459,\n",
                        "        28705, 28740,  1096,   368,   873,   574,  1948, 28713,  1656,   684,\n",
                        "          368, 28725,   304,   368,  1032,   369,   590, 28809,   267,   776,\n",
                        "         2548,  4392,  1722, 28723,  1824,  1235, 28705, 28770,   913,   737,\n",
                        "        28804,  2188, 28747, 28743,  3428, 28747,   382,  3221, 28719, 28723,\n",
                        "          842,   842, 28705,  1684,   315, 28809, 28719,   438, 28705, 28770,\n",
                        "        28725,   315, 28809,   584,  1073,  4357,   315, 28809, 28719,   459,\n",
                        "         1404,   298,   625,   778,  7598, 28723,   733, 28748, 16289, 28793,\n",
                        "        28738,   663,   377,   392, 28747, 19393, 28804,  1824,   622,   369,\n",
                        "          913,   737, 28804,  1047,   315,   654,   298,  1032,   368,  1404,\n",
                        "          778,   574,  9585,  3667, 28725,   910,   682,   315,   873,   368,\n",
                        "          460,   438, 28705, 28770, 28804,     2,   733, 16289, 28793,  3198,\n",
                        "        28708, 28747,   315,   949, 28809, 28707,   873, 28723,   851,   349,\n",
                        "         1856, 28723,   315, 28809, 28715,   776,  1601,  1873, 28723, 28705,\n",
                        "         4673, 28725,   513,   368,  2580,   528,  1873, 28725,  4357,   368,\n",
                        "        28809, 28715,  1032,   528, 14088,   288,   586, 28217, 28723,  1047,\n",
                        "          315,  2580, 16870, 28723,   330,   606,   291,  3049,  6003, 28809,\n",
                        "        28707,  1987,   579,   315,  4048, 28809, 28707,   625,   778,  7598,\n",
                        "        28723,   733, 28748, 16289, 28793, 28738,   663,   377,   392, 28747,\n",
                        "        24527, 28723,  9377,   369,  4804,  4662,  1055, 28804,     2,   733,\n",
                        "        16289, 28793,  3198, 28708, 28747,  5592, 28723,  1684,   315,   873,\n",
                        "          630, 11638, 28809, 28707,  1987, 28723,   733, 28748, 16289, 28793,\n",
                        "        28738,   663,   377,   392, 28747,   351, 28809, 14112, 28725,  2997,\n",
                        "          473, 28747,   863,   368,   873,   369,  3198, 28708, 12477,   369,\n",
                        "         4662, 28804,     2,   733, 16289, 28793,  2997,   473, 28747,  1770,\n",
                        "        28725,   315,  1539, 28809, 28707,   873,   369, 28723,   315,  5102,\n",
                        "          315, 28809,   333,   750,  9960,   680,  4501,   298,   739,   272,\n",
                        "         8626,  6470,   304,   351, 28809, 14112,   349, 26658,   821,   739,\n",
                        "          630,  2368, 28809, 28707,  1034, 28723, 28755, 28809, 14112, 28747,\n",
                        "          315,   553,   708,  3028,  3198, 28708,  2672,   369, 28723,   315,\n",
                        "          553,   708,  3028,   630, 12477,   272,  5133, 28723,   733, 28748,\n",
                        "        16289, 28793, 28738,   663,   377,   392, 28747,   382,  3221, 28719,\n",
                        "        28723,   842,   842,  6926,   511,   368,  1073,   368,   989,   622,\n",
                        "          347,   739,  3198, 28708,   349,  7887,   298, 28705, 28770, 28804,\n",
                        "        28705, 18862, 28723, 15573,   622,   347,  1873, 28723,   315, 28809,\n",
                        "          584,  1601,  1873, 28723,   315,  1601,  1873,  2141, 28723,     2,\n",
                        "          733, 16289, 28793,  2997,   473, 28747,  3198, 28708, 28725,   511,\n",
                        "          368,  1073,   368,   873,   767,   368,   927,   298,   511,   579,\n",
                        "        16870, 28723,   330,   606,   291,  3157, 28809, 28707,  7245,   438,\n",
                        "          368, 28804, 28743,  3428, 28747,  5592, 28723,   315,   927,   298,\n",
                        "         1943,   297,   586,  7278,   304,   459,  1985,   298,  2663,   739,\n",
                        "          630, 28809, 28713,  4434, 28723,   733, 28748, 16289, 28793]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
                        "        1, 1, 1])\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"input 入trl的 数据分离函数还有用于做 PLM\"\"\"\n",
                "\"\"\"测试PLM和自动标注attention的效果\"\"\"\n",
                "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
                "response_template = '[/INST]'\n",
                "instruction_template = '[INST]'\n",
                "# 自动将位于此区间的文本做PLW，这一点非常重要！！！！\n",
                "data_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer)\n",
                "# data_collator = DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
                "        # )\n",
                "dataloader = torch.utils.data.DataLoader(dataset=trl_test_dataset, \n",
                "                                         collate_fn=data_collator, \n",
                "                                         batch_size=10\n",
                "                                        )\n",
                "for batch in dataloader:\n",
                "    # 每个batch 都有三列 分别为\n",
                "    print(batch)\n",
                "    torch.set_printoptions(profile=\"full\")#将 torch 充分打印的工具\n",
                "    # print(x) # prints the whole tensor\n",
                "    print(batch['labels'][0],batch['input_ids'][0],batch['attention_mask'][0])\n",
                "    torch.set_printoptions(profile=\"default\") # 将 torch 显示形式还原\n",
                "    break"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'_continue_training'"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset_arg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(<IntervalStrategy.STEPS: 'steps'>,\n",
                            " '/home/ckqsudo/code2024/Mistral-7b-finetune/trl_standard_output/fuck8_6_8_unpacking__continue_training')"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from transformers import TrainingArguments\n",
                "from trl import SFTConfig\n",
                "\n",
                "sftConfig = SFTConfig(\n",
                "    packing=False,\n",
                "    bf16=True,\n",
                "    bf16_full_eval=True,\n",
                "    dataset_batch_size=8,\n",
                "    logging_strategy=\"steps\",\n",
                "    logging_dir=f\"/home/ckqsudo/code2024/Mistral-7b-finetune/trl_standard_output/fuck8_6_8_unpacking_{dataset_arg}/log\",\n",
                "    output_dir=f\"/home/ckqsudo/code2024/Mistral-7b-finetune/trl_standard_output/fuck8_6_8_unpacking_{dataset_arg}\", # directory to save and repository id\n",
                "    num_train_epochs=5,                     # number of training epochs\n",
                "    per_device_train_batch_size=2,          # batch size per device during training\n",
                "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
                "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
                "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
                "    save_strategy=\"steps\",                  # save checkpoint every epoch\n",
                "    save_steps=20,\n",
                "    learning_rate=5e-5,                     # learning rate, based on QLoRA paper\n",
                "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
                "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
                "    lr_scheduler_type=\"cosine\",           # use constant learning rate scheduler\n",
                "    push_to_hub=False,                       # push model to hub\n",
                "    report_to=\"tensorboard\",               # report metrics to tensorboard\n",
                "    local_rank=2,\n",
                "    remove_unused_columns=False,\n",
                "    max_seq_length=4096,\n",
                "    do_train=True, \n",
                "    do_eval=True,\n",
                "    evaluation_strategy=\"steps\",\n",
                "    eval_steps=10,\n",
                "    eval_packing=False,\n",
                "    \n",
                "    logging_steps=10,                       # log every 10 steps\n",
                "    dataset_kwargs={\n",
                "        \"add_special_tokens\": False,  # We template with special tokens\n",
                "        \"append_concat_token\": False, # No need to add additional separator token 这两个是最后新加的\n",
                "    },\n",
                "    )\n",
                "sftConfig.evaluation_strategy,sftConfig.output_dir# 这一步必须有"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model.config.use_cache = False\n",
                "# trainer.train(resume_from_checkpoint=\"/content/latest_checkpoint/\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ckqsudo/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
                        "\n",
                        "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
                        "  warnings.warn(message, FutureWarning)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ckqsudo/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_batch_size` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
                        "  warnings.warn(\n",
                        "/home/ckqsudo/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
                        "  warnings.warn(\n",
                        "/home/ckqsudo/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:382: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "import token\n",
                "from typing import Literal\n",
                "from trl import SFTTrainer\n",
                "from trl.trainer import ConstantLengthDataset, DataCollatorForCompletionOnlyLM\n",
                "max_seq_length = 4096 # max sequence length for model and packing of the dataset\n",
                "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
                "tokenizer.padding_side  = 'left'\n",
                "trainer = SFTTrainer(\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=data_collator,\n",
                "    packing=False,\n",
                "    model=model,\n",
                "    args=sftConfig,\n",
                "    # save_embedding_layers=True,\n",
                "    dataset_text_field='input_ids',\n",
                "    train_dataset=trl_train_dataset,\n",
                "    eval_dataset=trl_test_dataset,\n",
                "    peft_config=peft_config,\n",
                ")\n",
                "trainer.can_return_loss=True\n",
                "# https://discuss.huggingface.co/t/no-log-for-validation-loss-during-training-with-trainer/40094/2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'<bound method Trainer._get_output_dir of <trl.trainer.sft_trainer.SFTTrainer object at 0x75051f59e9a0>>'"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "str(trainer._get_output_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
                        "../aten/src/ATen/native/cuda/Indexing.cu:1292: indexSelectLargeIndex: block: [134,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start training, the model will be automatically saved to the hub and the output directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history[(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:425\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 425\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2757\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/accelerate/utils/operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/accelerate/utils/operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/peft/peft_model.py:537\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m    534\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;124;03m    Forward pass of the model.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:1053\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1066\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/models/mistral/modeling_mistral.py:905\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    898\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are attempting to perform batched generation with padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    899\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    900\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m call `tokenizer.padding_side  = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` before tokenizing the input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m         )\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# 2d mask is passed through the layers\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# 4d mask is passed through the layers\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask(\n\u001b[1;32m    909\u001b[0m         attention_mask,\n\u001b[1;32m    910\u001b[0m         (batch_size, seq_length),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         sliding_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msliding_window,\n\u001b[1;32m    914\u001b[0m     )\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/_tensor.py:1059\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1056\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1057\u001b[0m ):\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m)\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1063\u001b[0m )\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
                    ]
                }
            ],
            "source": [
                "# start training, the model will be automatically saved to the hub and the output directory\n",
                "trainer.train()\n",
                "trainer.save_model()\n",
                "# assert trainer.state.log_history[(-1)][\"train_loss\"] is not None\n",
                "# assert trainer.state.log_history[0][\"eval_loss\"] is not None\n",
                "# save model \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(sftConfig,data_collator,tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 251,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(733, 16289)"
                        ]
                    },
                    "execution_count": 251,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "ckq_dataset['input_ids'][1][2],ckq_dataset['input_ids'][1][3]\n",
                "# 打印向量"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 205,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 269,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('left', 0, '<unk>', 0)"
                        ]
                    },
                    "execution_count": 269,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "tokenizer.padding_side,tokenizer.pad_token_id,tokenizer.pad_token,tokenizer.unk_token_id"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyError",
                    "evalue": "'images'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start training, the model will be automatically saved to the hub and the output directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history[(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:425\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 425\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/transformers/trainer.py:1821\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1818\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1820\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1821\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1822\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/accelerate/data_loader.py:451\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
                        "File \u001b[0;32m~/sys_tool/miniconda/tmp/yes/envs/trl/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[91], line 26\u001b[0m, in \u001b[0;36mLLavaDataCollator.__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# batch=self.tokenize_messages(messages, tokenizer, mask_inputs=True)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# batch_list.append(batch)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m---> 26\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     28\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(texts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n",
                        "\u001b[0;31mKeyError\u001b[0m: 'images'"
                    ]
                }
            ],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 270,
            "metadata": {},
            "outputs": [],
            "source": [
                "def conversations_formatting_function(tokenizer: AutoTokenizer, messages_field: Literal[\"messages\", \"conversations\"]):\n",
                "    r\"\"\"\n",
                "    更加鲁棒的函数，能对列表进行遍历 查找messages 和conversations是否有messages信息\n",
                "    \"\"\"\n",
                "    def format_dataset(examples):\n",
                "        if isinstance(examples[messages_field][0], list):\n",
                "            output_texts = []\n",
                "            for i in range(len(examples[messages_field])):\n",
                "                output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))\n",
                "            return output_texts\n",
                "        else:\n",
                "            return tokenizer.apply_chat_template(examples[messages_field], tokenize=False) \n",
                "    return format_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 206,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[]"
                        ]
                    },
                    "execution_count": 206,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "trainer.state.log_history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### 以后就不用那么麻烦了，直接计算参数！！！"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(preds, targets):\n",
                "    assert len(preds) == len(targets), (\n",
                "        \"The length of pred_responses should be equal to the length of \"\n",
                "        \"target_responses. But received {} and {}.\".format(len(preds), len(targets))\n",
                "    )\n",
                "    rouge = Rouge()\n",
                "    bleu4 = BLEU(n_size=4)\n",
                "    scores = []\n",
                "    for pred, target in zip(preds, targets):\n",
                "        try:\n",
                "            score = rouge.get_scores(\" \".join(pred), \" \".join(target))\n",
                "            scores.append([score[0][\"rouge-1\"][\"f\"], score[0][\"rouge-2\"][\"f\"], score[0][\"rouge-l\"][\"f\"]])\n",
                "        except ValueError:\n",
                "            scores.append([0, 0, 0])\n",
                "        bleu4.add_inst(pred, [target])\n",
                "    rouge1 = np.mean([i[0] for i in scores])\n",
                "    rouge2 = np.mean([i[1] for i in scores])\n",
                "    rougel = np.mean([i[2] for i in scores])\n",
                "\n",
                "    rouge1 = round(rouge1, 4)\n",
                "    rouge2 = round(rouge2, 4)\n",
                "    rougel = round(rougel, 4)\n",
                "    bleu4 = round(bleu4.score(), 4)\n",
                "    return dict(\n",
                "        rouge1=rouge1,\n",
                "        rouge2=rouge2,\n",
                "        rougel=rougel,\n",
                "        bleu4=bleu4,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"一个老哥写的能在做PWL的时候做packing的函数\"\"\"\n",
                "\"\"\"https://github.com/huggingface/trl/issues/632#top\"\"\"\n",
                "\"\"\"https://arxiv.org/abs/2401.13586 PLM真的重要的吗论文\"\"\"\n",
                "def get_assistant_start_end_indices(messages, conversation_text):\n",
                "    start_indices = []\n",
                "    current_index = 0\n",
                "    for message in messages:\n",
                "        message_text = message[\"content\"]\n",
                "        match_index = conversation_text[current_index:].find(message_text)\n",
                "        start_indices.append(current_index + match_index)\n",
                "        current_index += match_index + len(message_text)\n",
                "    end_indices = [len(conversation_text) if i == len(start_indices) - 1 else start_indices[i+1] for i, x in enumerate(start_indices)]\n",
                "    roles = [message[\"role\"] for message in messages]\n",
                "    return [(s, e) for s, e, r in zip(start_indices, end_indices, roles) if r == \"assistant\"]\n",
                "\n",
                "def get_masked_labels(conversation_ids, assistant_ranges):\n",
                "    labels = []\n",
                "    for id_, (id_s, id_e) in list(zip(conversation_ids[\"input_ids\"], conversation_ids[\"offset_mapping\"])):\n",
                "        if any(id_s >= s and id_e <= e for s, e in assistant_ranges):\n",
                "            yield id_\n",
                "        else:\n",
                "            yield -100\n",
                "\n",
                "def tokenize_messages(messages, tokenizer, mask_inputs=True):\n",
                "    conversation_text = tokenizer.apply_chat_template(conversation=messages, add_generation_prompt=True, tokenize=False)\n",
                "    conversation_ids = tokenizer(conversation_text, return_offsets_mapping=mask_inputs)\n",
                "    if mask_inputs:\n",
                "        assistant_ranges = get_assistant_start_end_indices(messages, conversation_text)\n",
                "        labels = get_masked_labels(conversation_ids, assistant_ranges)\n",
                "        conversation_ids[\"labels\"] = list(labels)\n",
                "        \n",
                "        del conversation_ids[\"offset_mapping\"]\n",
                "    else:\n",
                "        conversation_ids[\"labels\"] = conversation_ids[\"input_ids\"]\n",
                "    return conversation_ids\n",
                "# ckq_dataset = train_dataset['train'].map(\n",
                "#     lambda examples: {\n",
                "#         'input_ids': tokenize_messages(messages=examples['messages'], tokenizer=tokenizer, mask_inputs=True)\n",
                "#     }\n",
                "# )\n",
                "ckq_dataset = train_dataset['train'].map(\n",
                "    lambda examples: {\n",
                "        'text': tokenizer.apply_chat_template(conversation=examples['messages'], add_generation_prompt=False, tokenize=False)\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
